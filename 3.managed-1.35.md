#### Disabling SWAP
```bash
sudo swapoff -a
sed -e '/swap/s/^/#/g' -i /etc/fstab
```

#### Loading Kernel Modules
```bash
tee /etc/modules-load.d/modules.conf <<EOF
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sudo sysctl --system
```

#### Set Kubernetes and CRI-O Version
```bash
KUBERNETES_VERSION=v1.35
CRIO_VERSION=v1.35
```

#### Install all required packages
```bash
apt-get update -y
apt-get install -y software-properties-common curl

curl -fsSL https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/Release.key |
    gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/ /" |
    tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://download.opensuse.org/repositories/isv:/cri-o:/stable:/$CRIO_VERSION/deb/Release.key |
    gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg

echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://download.opensuse.org/repositories/isv:/cri-o:/stable:/$CRIO_VERSION/deb/ /" |
    tee /etc/apt/sources.list.d/cri-o.list

sudo apt-get update -y
sudo apt-get install -y cri-o cri-tools kubelet kubeadm kubectl -y

sudo apt-mark hold kubelet kubeadm kubectl
sudo systemctl start crio.service
```

#### Verify installations
```bash
kubeadm version
kubelet --version
kubectl version --client
```

#### Creating cluster configuration file `kubeadm-config.yaml`
```bash
cat <<EOF > kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration

clusterName: kubernetes
kubernetesVersion: v1.35.0

# Load balancer / VIP (MANDATORY for HA)
controlPlaneEndpoint: "192.168.29.100:6443"

networking:
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/16

# External etcd
etcd:
  external:
    endpoints:
      - https://etcdone:2379
      - https://etcdtwo:2379
      - https://etcdthree:2379
    caFile: /etc/kubernetes/pki/etcd/ca.pem
    certFile: /etc/kubernetes/pki/etcd/apiserver-etcd-client.pem
    keyFile: /etc/kubernetes/pki/etcd/apiserver-etcd-client-key.pem

# API server certificate SANs
apiServer:
  certSANs:
    - "192.168.29.81"
    - "192.168.29.100"
    - "masterone"
    - "192.168.29.51"
    - "127.0.0.1"
    - "kubernetes"
    - "kubernetes.default"
    - "kubernetes.default.svc"
    - "kubernetes.default.svc.cluster.local"

    #- "" #<HAPROXY_PUBLIC_IP>

---
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration

localAPIEndpoint:
  advertiseAddress: 192.168.29.51   # MASTERONE IP
  bindPort: 6443

nodeRegistration:
  name: masterone
  criSocket: unix:///var/run/crio/crio.sock

---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration

serializeImagePulls: false
maxParallelImagePulls: 10

evictionHard:
  memory.available: "200Mi"
  nodefs.available: "20%"
  nodefs.inodesFree: "5%"
EOF
```

#### Pulling configuration images and Starting Cluster using kubeadm init
```bash
sudo kubeadm config images pull
sleep 5
kubeadm init --config kubeadm-config.yaml --upload-certs --ignore-preflight-errors=all
sleep 20
```
In case of reset the control plane
```bash
kubeadm reset -f
rm -rf /etc/kubernetes /var/lib/etcd
systemctl restart crio
systemctl restart kubelet
```

### **Note:**
**Use the respective kubeadm join commands you copied from the output of kubeadm init command on the first master.**

**IMPORTANT: Don't forget the --apiserver-advertise-address `<The local IP of the node you are joining>` option to the join command when you join the other master nodes.Z**

```bash
 kubeadm join 192.168.29.100:6443 --token 3dd75m.357oardeo93jsmp6 \
        --discovery-token-ca-cert-hash sha256:532f6ce4998198b64a08c3c7fcac941f3b059aa9a60c8cd42583631ed64cde1c \
        --control-plane --certificate-key 15c75dbca6500dba3bd0bce5b20a46a6f97836da98925562f20711adf58442b6 \
        --apiserver-advertise-address 192.168.29.51
```

#### Generate a Joining command
```bash
# On Lead Master Node
kubeadm token create --print-join-command --certificate-key \
$(kubeadm init phase upload-certs --upload-certs | tail -1) #For other master nodes

kubeadm token create --print-join-command #For worker nodes
```

#### Join token for other control-plane nodes
```bash
kubeadm join <load-balancer-private-ip>:6443 --token <token> \
--discovery-token-ca-cert-hash sha256:<hash> \
--control-plane --certificate-key <certificate-key> --apiserver-advertise-address 192.168.29.51
```

#### Setting up home directory and copy `/etc/kubernetes/admin.conf` into `~/.kube/config`
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### Installing Calico as CNI
```bash
kubectl apply -f https://docs.tigera.io/calico/latest/manifests/calico.yaml
#or 
kubectl --kubeconfig=/etc/kubernetes/admin.conf create -f https://docs.projectcalico.org/v3.18/manifests/calico.yaml
```

#### Generate a Joining command
```bash
kubeadm token create --print-join-command #For worker nodes
```

#### Join token for worker nodes
```bash
kubeadm join <master-node-hostname>:6443 --token <token> \
--discovery-token-ca-cert-hash sha256:<hash>
```

#### Check control plane node status:
```bash
kubectl cluster-info
kubectl get nodes
kubectl get nodes -o wide
kubectl get pods --all-namespaces
```

#### Check from ETCD nodes
```bash
unset ETCDCTL_ENDPOINTS
unset ETCDCTL_CACERT
unset ETCDCTL_CERT
unset ETCDCTL_KEY

export ENDPOINTS=https://etcdone:2379,https://etcdtwo:2379,https://etcdthree:2379
export ETCDCTL_CACERT=/etc/etcd/pki/ca.pem
export ETCDCTL_CERT=/etc/etcd/pki/apiserver-etcd-client.pem
export ETCDCTL_KEY=/etc/etcd/pki/apiserver-etcd-client-key.pem

etcdctl --endpoints=$ENDPOINTS get / --prefix --keys-only
```
```bash
root@masterone:~# kubeadm init --config kubeadm-config.yaml --upload-certs --ignore-preflight-errors=all
[init] Using Kubernetes version: v1.35.0
[preflight] Running pre-flight checks
        [WARNING ExternalEtcdVersion]: this version of kubeadm only supports external etcd version >= 3.5.24-0. Current version: 3.5.21
        [WARNING ExternalEtcdVersion]: this version of kubeadm only supports external etcd version >= 3.5.24-0. Current version: 3.5.21
        [WARNING ExternalEtcdVersion]: this version of kubeadm only supports external etcd version >= 3.5.24-0. Current version: 3.5.21
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local masterone] and IPs [10.96.0.1 192.168.29.51 192.168.29.100 192.168.29.81 127.0.0.1]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
[patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 509.810319ms
[control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
[control-plane-check] Checking kube-apiserver at https://192.168.29.51:6443/livez
[control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
[control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
[control-plane-check] kube-controller-manager is healthy after 2.509221504s
[control-plane-check] kube-scheduler is healthy after 6.762062356s
[control-plane-check] kube-apiserver is healthy after 26.082968203s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
15c75dbca6500dba3bd0bce5b20a46a6f97836da98925562f20711adf58442b6
[mark-control-plane] Marking the node masterone as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node masterone as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 3dd75m.357oardeo93jsmp6
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes running the following command on each as root:

  kubeadm join 192.168.29.100:6443 --token 3dd75m.357oardeo93jsmp6 \
        --discovery-token-ca-cert-hash sha256:532f6ce4998198b64a08c3c7fcac941f3b059aa9a60c8cd42583631ed64cde1c \
        --control-plane --certificate-key 15c75dbca6500dba3bd0bce5b20a46a6f97836da98925562f20711adf58442b6

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.29.100:6443 --token 3dd75m.357oardeo93jsmp6 \
        --discovery-token-ca-cert-hash sha256:532f6ce4998198b64a08c3c7fcac941f3b059aa9a60c8cd42583631ed64cde1c

```
```bash
root@masterthree:~#      kubeadm join 192.168.29.100:6443 --token 3dd75m.357oardeo93jsmp6 \
        --discovery-token-ca-cert-hash sha256:532f6ce4998198b64a08c3c7fcac941f3b059aa9a60c8cd42583631ed64cde1c \
        --control-plane --certificate-key 15c75dbca6500dba3bd0bce5b20a46a6f97836da98925562f20711adf58442b6 \
        --apiserver-advertise-address 192.168.29.53

[preflight] Running pre-flight checks
[preflight] Reading configuration from the "kubeadm-config" ConfigMap in namespace "kube-system"...
[preflight] Use 'kubeadm init phase upload-config kubeadm --config your-config-file' to re-upload it.
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[download-certs] Saving the certificates to the folder: "/etc/kubernetes/pki"
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local masterone masterthree] and IPs [10.96.0.1 192.168.29.53 192.168.29.81 192.168.29.51 127.0.0.1]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
[patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[control-plane-join] Using external etcd - no local stacked instance added
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.504110672s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap
[mark-control-plane] Marking the node masterthree as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node masterthree as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
[control-plane-check] Checking kube-apiserver at https://192.168.29.53:6443/livez
[control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
[control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
[control-plane-check] kube-controller-manager is healthy after 23.507531ms
[control-plane-check] kube-scheduler is healthy after 40.963073ms
[control-plane-check] kube-apiserver is healthy after 2.502955256s

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
```

